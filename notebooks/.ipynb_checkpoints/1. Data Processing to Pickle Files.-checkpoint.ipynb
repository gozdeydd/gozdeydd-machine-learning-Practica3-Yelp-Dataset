{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008fae8c",
   "metadata": {},
   "source": [
    "\n",
    "# CUNEF MUCD 2022/2023\n",
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f53fe9",
   "metadata": {},
   "source": [
    "## Analisis of Yelp Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ca8f2",
   "metadata": {},
   "source": [
    "In this notebook we open yelp datasets to be used in other analisis and save it to pickle. \n",
    "\n",
    "Due to size, and format we have faced several problems. Sometimes kernel dies in the middle of execution, even though sometimes not. Recomendation to follow other notebooks is to loading end pickle files created in this notebook. \n",
    "\n",
    "In order to reach end pickle files,several pickle files are created after every execution wherever we see problem. Later pickles are loaded and concatanated to each other with pd.concat. Execution can differ from machine to machine, if you face similar problems you need refresh the notebook and execute from the place you have been, you can go on reading and saving the pickles. If there are more problems to even before saving the pickle, recomendation would be dividing into more smaler pieces and do the same. We are saving every small output so we don't have to execute back those steps if the kernel dies.\n",
    "\n",
    "Although we could have put some loops, in order to have more control and be able to save all files we have avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ec326",
   "metadata": {},
   "source": [
    "### 1. Importing necessary libraries and defining file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d0a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6619f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining libraries to read and write data  \n",
    "\n",
    "path_raw = '../data/raw/'\n",
    "path_processed = '../data/processed/'\n",
    "path_review = '../data/processed/review'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2573255a",
   "metadata": {},
   "source": [
    "### 2.Reading Business file and saving to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading business dataset and writing it to pickle.\n",
    "business = pd.read_json(Path(path_raw, 'yelp_academic_dataset_business.json'), lines=True)\n",
    "business.to_pickle(Path(path_processed, 'business.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d3be4",
   "metadata": {},
   "source": [
    "### 3. Reading Checkin file and saving to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading checkin dataset and writing it to pickle\n",
    "checkin = pd.read_json(Path(path_raw, 'yelp_academic_dataset_checkin.json'), lines=True)\n",
    "checkin.to_pickle(Path(path_processed, 'checkin.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e77ed55",
   "metadata": {},
   "source": [
    "### 4. Reading Review file and saving to pickle\n",
    "\n",
    "Since this file is one of the biggest, we follow a different strategy. Here we divide in small pickle files. Chunksize has choosen to 500000, the size we can optimally work. In other machines this could be changed to execute in a better performance.\n",
    "\n",
    "After execution, sometimes gives error. since python is executing in order, till we recieve error, we are able to save our pickle file. For this reason it is still functional and we keep like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c890ae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we read small chunks and save it to pickle. \n",
    "\"\"\"In the end of the execution it may give error but since we already saved small pickles \n",
    "we are able to have our dataframe\"\"\"\n",
    "\n",
    "iter_review = pd.read_json(Path(path_raw, 'yelp_academic_dataset_review.json'), lines=True, chunksize = 500000)\n",
    "\n",
    "diccionary = {}\n",
    "i = 0\n",
    "for caso in iter_review:\n",
    "    caso.to_pickle('../data/processed/review/rev' +str(i)+'.pickle')\n",
    "    i = i + 1\n",
    "# with or without error, there apperars new pickle files from rev0 to rev12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928b995",
   "metadata": {},
   "source": [
    "In below cells we are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8736bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review/rev0.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        rev0 = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review/rev1.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b777859",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review/rev2.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ad404",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review/rev3.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d9d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review/rev4.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review/rev5.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev5 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded143d4",
   "metadata": {},
   "source": [
    "Since we are not able to save all 13 files together we also save in small pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review1 = pd.concat([rev0, rev1, rev2, rev3, rev4, rev5])\n",
    "review1.to_pickle(Path(path_processed, 'review1.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d695c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review/rev6.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev6 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review/rev7.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev7 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../data/processed/review/rev8.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev8 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc2b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "with open('../data/processed/review/rev9.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev9 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "with open('../data/processed/review/rev10.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev10 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a968e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review/rev11.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev11= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3dfd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review/rev12.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    rev12= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review2 = pd.concat([rev6, rev7, rev8, rev9, rev10])\n",
    "review2.to_pickle(Path(path_processed, 'review2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "review3 = pd.concat([rev11,rev12])\n",
    "review3.to_pickle(Path(path_processed, 'review3.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f16a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review1.pkl', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    review1= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review2.pkl', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    review2= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd16541",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/review3.pkl', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "    review3= pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba891a12",
   "metadata": {},
   "source": [
    "Finally we use secondary unions to final union below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51230406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review = pd.concat([review1, review2,review3])\n",
    "review.to_pickle(Path(path_processed, 'review.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f2a3b",
   "metadata": {},
   "source": [
    "### 5. Reading Tip file and saving to pickle\n",
    "\n",
    "Tip file is saved without big problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676da982",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip = pd.read_json(Path(path_raw, 'yelp_academic_dataset_tip.json'), lines=True)\n",
    "tip.to_pickle(Path(path_processed, 'tip.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c76dac",
   "metadata": {},
   "source": [
    "### 6. Reading User file and saving to Pickle\n",
    "\n",
    "We face similar problems we had in the review file. Same strategies with review file are followed. Optimum chunksize for this case is set to 100000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we read small chunks and save it to pickle. \n",
    "\"\"\"In the end of the execution it may give error but since we already saved small pickles \n",
    "we are able to have our dataframe\"\"\"\n",
    "\n",
    "iter_user = pd.read_json(Path(path_raw, 'yelp_academic_dataset_user.json'), lines=True, chunksize = 100000)\n",
    "\n",
    "diccionary = {}\n",
    "i = 0\n",
    "for caso in iter_user:\n",
    "    caso.to_pickle('../data/processed/user/u' +str(i)+'.pickle')\n",
    "    i = i + 1\n",
    "# with or without error, there apperars new pickle files from u0 to u19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cecd97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u0.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u0 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1215b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u1.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u1 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd2cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u2.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u2 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f92715",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u3.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u3 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ddea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first union of 4 pickles\n",
    "user1 = pd.concat([u0, u1, u2, u3])\n",
    "user1.to_pickle(Path(path_processed, 'user1.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9205e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u4.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u4 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39a4aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u5.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u5 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2955500",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u6.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u6 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66dc925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u7.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u7 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43d969c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second union of next union of 4 pickles\n",
    "user2 = pd.concat([u4, u5, u6, u7])\n",
    "user2.to_pickle(Path(path_processed, 'user2.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b37edd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u8.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u8 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f5b6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u9.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u9 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79322359",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u10.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u10 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e5db3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u11.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u11 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d472c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#third union of next union of 4 pickles\n",
    "user3 = pd.concat([u8, u9, u10, u11])\n",
    "user3.to_pickle(Path(path_processed, 'user3.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f35204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u12.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u12 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce3e74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u13.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u13 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e233099",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u14.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u14 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af991f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u15.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u15 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a19286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forth union of next union of 4 pickles\n",
    "user4 = pd.concat([u12, u13, u14, u15])\n",
    "user4.to_pickle(Path(path_processed, 'user4.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f16ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u16.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u16 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "077f5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u17.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u17 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8d4068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u18.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u18 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8d57c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/user/u19.pickle', 'rb') as f:\n",
    "    # Load the data from the file\n",
    "        u19 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b0be6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fifth union of next union of 4 pickles\n",
    "user5 = pd.concat([u16, u17, u18, u19])\n",
    "user5.to_pickle(Path(path_processed, 'user5.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c26796d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final union\n",
    "user = pd.concat([user1, user2, user3, user4, user5])\n",
    "user.to_pickle(Path(path_processed, 'user.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b943c",
   "metadata": {},
   "source": [
    "We have processed all files and saved them in the archives. We are ready to do the next steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practica_yelp",
   "language": "python",
   "name": "practica_yelp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
